{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "04_article_vector_w2v_v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCvPvoA-9VBV"
      },
      "source": [
        "# to read article from MySQL DATABASE: contentanalysis, TABLE: articles\n",
        "def get_id_from_mysql(sql_database, sql_table, column_name):\n",
        "    import mysql.connector\n",
        "    sql_db = mysql.connector.connect(\n",
        "        host = '127.0.0.1',\n",
        "        user = 'root',\n",
        "        password = \"\",\n",
        "        database = sql_database\n",
        "    )\n",
        "    cursor = sql_db.cursor()\n",
        "    \n",
        "    # to check whether TABLE EXISTS OR NOT\n",
        "    cursor.execute(\"SHOW TABLES LIKE '{}'\".format(sql_table))\n",
        "    temp_result = cursor.fetchone()\n",
        "    if (temp_result):\n",
        "        logging.info(\"TABLE {} exists in DATABASE {}\".format(sql_table, sql_database))\n",
        "    else:\n",
        "        logging.info(\"TABLE {} exists in DATABASE {}\".format(sql_table, sql_database))\n",
        "        \n",
        "    select_sql = \"SELECT {} FROM {}\".format(column_name[0], sql_table)\n",
        "    cursor.execute(select_sql)\n",
        "    result = cursor.fetchall()\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzZ2Ha3Y9VBX"
      },
      "source": [
        "def get_mean_article_vector(w2v_model, c_cut, c_id):\n",
        "    text_words = c_cut.split()\n",
        "    \n",
        "    sentence_vec = [0.0]*100\n",
        "    #print(text_words)\n",
        "    word_count = 0\n",
        "    \n",
        "    # if there is words in text_words,sum up its word_vector as article vector and take its average\n",
        "    if len(text_words) is not 0:\n",
        "        for word in text_words:\n",
        "            try:\n",
        "                word_vec = w2v_model.wv[word]\n",
        "                word_count += 1\n",
        "                sentence_vec = [x+y for x, y in zip(sentence_vec, word_vec)]\n",
        "            except KeyError:\n",
        "                logging.info(\"KeyError Handling: no words {} in {}\".format(word, c_id))\n",
        "                continue\n",
        "        #length = len(text_words)\n",
        "        length = word_count\n",
        "        \n",
        "        sentence_vec = [round(value/length, 5) for value in sentence_vec]\n",
        "    else: # if there is no words in the text_words, assign value as -100\n",
        "        sentence_vec = [-100.0]*100\n",
        "    #print(\"length:ã€€\" , word_count, len(text_words))\n",
        "    return sentence_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QuiRVsDI9VBY"
      },
      "source": [
        "import logging\n",
        "logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s', level=logging.INFO)\n",
        "\n",
        "# to get the cut_corpus each article from the txt file\n",
        "corpus = []\n",
        "#with open('advertisement_article_cut.txt', 'r', encoding='utf-8') as content:\n",
        "with open('advertisement_article_cut_0617.txt', 'r', encoding='utf-8') as content:\n",
        "    logging.info(\"start to cut words\")\n",
        "    \n",
        "    sql_database = \"contentanalysis\"\n",
        "    sql_table = \"articles\"\n",
        "    column_name = [\"id\", \"content\"]\n",
        "    content_id = get_id_from_mysql(sql_database, sql_table, column_name)  \n",
        "    \n",
        "    id_count = 0\n",
        "    \n",
        "    for text_num, line_str in enumerate(content):\n",
        "\n",
        "        line_str = line_str.replace('\\n', '')\n",
        "        line_str = line_str.strip()\n",
        "        content_list = [content_id[id_count][0], line_str]\n",
        "        corpus.append(content_list)\n",
        "        id_count += 1\n",
        "    print(corpus[:100])\n",
        "    print(len(corpus))\n",
        "        \n",
        "# create c_vecots_df to store all c_vector\n",
        "import pandas as pd\n",
        "column_name = ['content_id']\n",
        "for i in range(100):\n",
        "    temp_col = 'content_vec_{}'.format(str(i))\n",
        "    column_name.append(temp_col)\n",
        "#print(column_name)\n",
        "c_vectors_df = pd.DataFrame(columns=column_name)\n",
        "\n",
        "# to load the word2vec model\n",
        "from gensim.models import word2vec\n",
        "#model_name = \"ad_w2v_model_v1.model\"\n",
        "model_name = \"ad_w2v_model_v2_0617.model\"\n",
        "w2v_model = word2vec.Word2Vec.load(model_name)\n",
        "logging.info(\"loaded w2v model: {}\".format(model_name))\n",
        "\n",
        "for c_id, c_cut in corpus:\n",
        "    logging.info(\"sart create content_vec for {}\".format(str(c_id)))\n",
        "    \n",
        "    # to get the content vector\n",
        "    logging.info(\"start calculate article vector\")\n",
        "    c_vector = get_mean_article_vector(w2v_model, c_cut, c_id)\n",
        "    logging.info(\"end calculate article vector\")\n",
        "    \n",
        "    # to store the c_vector as a total dataframe: c_vectors_df\n",
        "    logging.info(\"start combine vectors\")\n",
        "    c_vector.insert(0, c_id)\n",
        "    c_df = pd.DataFrame([c_vector], columns=c_vectors_df.columns)\n",
        "    c_vectors_df = pd.concat([c_vectors_df, c_df], ignore_index=True)\n",
        "    logging.info(\"end combine vectors\")\n",
        "    logging.info(\"{} content_vec DONE\".format(str(c_id)))\n",
        "    \n",
        "\n",
        "# to store the result of content_vec as csv file\n",
        "c_vectors_df = c_vectors_df.drop_duplicates(subset=['content_id'], ignore_index=True)\n",
        "#csv_filename = 'content_vectors.csv'\n",
        "csv_filename = 'content_vectors_0617.csv'\n",
        "c_vectors_df.to_csv(csv_filename, index=False)\n",
        "#logging.info(\"have finished  create content vector and INSERT it INTO TABLE articles_test\")\n",
        "logging.info(\"have finished  create content vector and output is as {}\".format(csv_filename))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8esTyNOF9VBa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}