# Digital_content_revenue_prediction
A research project creates a digital advertisement revenue prediction model to help content providers leverage social media more efficiently.

## Background
Workinsa as a Data Science research assistant at Academia Sinica, I teamed up with [Szu-Chuang Li](http://www.ic.tku.edu.tw/index.php/component/sppagebuilder/page/50-tommy.html) to conduct another innovative research: Predicting Advertisement Revenue of Social Media DrivenContent Websites: Toward More Efficient and SustainableSocial Media Posting. We worked with a small-sized marketing company to get raw data from various data sources.

In the research project, I conducted feature engineering in image properties and word embedding of NLP. I also take information before posting as input features, such as fans of pages, posts category, and teimline of posting, to build up three machine learning classifiers: Decision Tree, XGBoost, and Deep Neuron Network, to predict digital advertisement supporeted by [Google AdSense](https://www.google.com/adsense/start/) plateform. To improve the performance of the classifiers, I deal with imbalanced issue of our customized labeled data set.

The research figure out the model that can minimize the amount of social media posting while retaining most of the advertisement revenue13and user engagement. The best model found is an XGBoost classification model, which can reduce 70% of the total posting number while maintaining 70% of the advertisement revenue. The model15can help a content website post much less and minimize the revenue loss due to less posting, which is beneficial for its continuous traffic redirection from social media platforms.

## Project Structure

Overall, Python is the main programming language for implementation and the result is run on Jupyter Notebook platform. I only present main part of projcet experiments as example.

In the file, [01_advertisement_word2vec.ipynb](https://github.com/ching870423/Digital_content_revenue_prediction/blob/main/01_advertisement_word2vec.ipynb), I extracted description in Chinese characters from original html code each post. Then, I trained a Word2Vec model using [gensim](https://radimrehurek.com/gensim/models/word2vec.html) library after tokenizing the whole corpus.

In the file, [02_article_vector_w2v_v2.ipynb](https://github.com/ching870423/Digital_content_revenue_prediction/blob/main/02_article_vector_w2v_v2.ipynb), I generated article vectors with dimension size 100 for each post. The method I implemented is that I got word embedding vectors of each word per post from the Word2Vec model I trained previously, and then averaged the word embedding vectors as article vectors each post.

In the file, [03_advertisement_bert_as_server_v2_list_export-512(tensorflow).ipynb](https://github.com/ching870423/Digital_content_revenue_prediction/blob/main/03_advertisement_bert_as_server_v2_list_export-512(tensorflow).ipynb), I conducted the second approach in NLP, word embdedding using BERT model. I loaded a pre-trained model, BERT-Base Chinese, and generated article vectors in dimension size 512 for each post using [bert-as-service](https://github.com/hanxiao/bert-as-service) API. Then, I exported the result as a csv file.

In the file, [04_bert_autoencoder(tensorflow_gpu).ipynb](https://github.com/ching870423/Digital_content_revenue_prediction/blob/main/04_bert_autoencoder(tensorflow_gpu).ipynb), I tried to downsize the 512-article vactors using [autoencoder build by Keras](https://blog.keras.io/building-autoencoders-in-keras.html) in order to solve the large dimension issue for the word embedding vectors generated by bert-as-service.

In the file, [05_advertisement_image_feature_googleAPI.ipynb](https://github.com/ching870423/Digital_content_revenue_prediction/blob/main/05_advertisement_image_feature_googleAPI.ipynb), I extracted features of main pictures, such as topic colors or labels, in each posts using [Google Vision API-Detect image properites](https://cloud.google.com/vision/docs/detecting-properties) and [Google Vision API-Detect labels](https://cloud.google.com/vision/docs/labels).

In the file, [06_advertisement_prediction_img_featues.ipynb](https://github.com/ching870423/Digital_content_revenue_prediction/blob/main/06_advertisement_prediction_img_featues.ipynb), I undersampled randomly using [RandomUnderSampler](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html) and oversampled using [SMOTE](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.html) before building prediction models. Then, I demonstrated how to applied our customized data set to 2 classifier: Decision Tree (using [scikit-learn library](https://scikit-learn.org/stable/modules/tree.html)) and XGboost (using [XGBoost library](https://xgboost.readthedocs.io/en/latest/python/python_intro.html)).

## Result
- List of columns used in classifiers

|Name of Variable|Description|
|---|---|
|WEEKDAY|the day in a week the date when the post is created|
|HOUR|the hour in a day the date when the post is created|
|CATEGORY|the category of the subject of the post|
|fans|number of fans of the Facebook page|
|ent0-28|if a keyword exists in the subject and first few words of apost,one-hot encoded|
|img_size_pixel|the width, height, and area of main image in posts|
|img_colorthe RGB color codes of topic colors for main image in posts|
|clicks|numnber clicks a post received|
|adsense_revenue|monetary revenue a post generated|
